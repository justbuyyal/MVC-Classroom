{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torchvision import datasets\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from torchvision.datasets import DatasetFolder\r\n",
    "from torchsummary import summary\r\n",
    "from PIL import Image\r\n",
    "from tqdm import tqdm\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "transform = transforms.Compose(\r\n",
    "    [transforms.RandomHorizontalFlip(0.4),\r\n",
    "     transforms.RandomVerticalFlip(),\r\n",
    "     transforms.ToTensor(),\r\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "batch_size = 4\r\n",
    "\r\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\r\n",
    "train_len = int(len(train_dataset) * 0.8)\r\n",
    "val_len = len(train_dataset) - train_len\r\n",
    "trainset, valset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\r\n",
    "\r\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\r\n",
    "\r\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\r\n",
    "\r\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import torch.nn.functional as F\r\n",
    "class CNN(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\r\n",
    "        self.batch_nor1 = nn.BatchNorm2d(6)\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\r\n",
    "        self.batch_nor2 = nn.BatchNorm2d(16)\r\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n",
    "        self.fc2 = nn.Linear(120, 84)\r\n",
    "        self.fc3 = nn.Linear(84, 10)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.pool(F.relu(self.batch_nor1(self.conv1(x))))\r\n",
    "        x = self.pool(F.relu(self.batch_nor2(self.conv2(x))))\r\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "model = CNN().to(device)\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss().to(device)\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\r\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\r\n",
    "\r\n",
    "Epoch = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import torchvision\r\n",
    "\r\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n",
    "# functions to show an image\r\n",
    "def imshow(img):\r\n",
    "    img = img / 2 + 0.5     # unnormalize\r\n",
    "    npimg = img.numpy()\r\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "\r\n",
    "# get some random training images\r\n",
    "dataiter = iter(train_loader)\r\n",
    "images, labels = dataiter.next()\r\n",
    "\r\n",
    "# show images\r\n",
    "imshow(torchvision.utils.make_grid(images))\r\n",
    "# print labels\r\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "for epoch in range(Epoch):\r\n",
    "    running_loss = 0.0\r\n",
    "    running_corrects = 0.0\r\n",
    "\r\n",
    "    val_loss = 0.0\r\n",
    "    val_corrects = 0.0\r\n",
    "    total = 0\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    for idx, data in enumerate(train_loader):\r\n",
    "      # get the input; data is a list of [inputs, labels]\r\n",
    "      inputs, labels = data\r\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\r\n",
    "\r\n",
    "      # zero the parameter gradients\r\n",
    "      optimizer.zero_grad()\r\n",
    "\r\n",
    "      # forward + backward + optimize\r\n",
    "      outputs = model(inputs)\r\n",
    "      loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "      loss.backward()\r\n",
    "      optimizer.step()\r\n",
    "\r\n",
    "      # print statistics\r\n",
    "      _, preds = torch.max(outputs, 1)\r\n",
    "      running_loss += loss.item()\r\n",
    "      running_corrects += torch.sum(preds == labels.data)\r\n",
    "      if idx % 2000 == 1999:\r\n",
    "        print('[Epoch: %d, %5d] loss: %.3f'%(epoch + 1, idx, running_loss / 2000))\r\n",
    "        running_loss = 0.0\r\n",
    "        running_corrects = 0.0\r\n",
    "    \r\n",
    "    # -------- Validation --------\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "      for idx, data in enumerate((val_loader)):\r\n",
    "        inputs, labels = data\r\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
    "        predict = model(inputs)\r\n",
    "        loss = criterion(predict, labels)\r\n",
    "\r\n",
    "        _, vals = torch.max(predict, 1)\r\n",
    "        val_loss += loss.item()\r\n",
    "        val_corrects += (vals == labels).sum().item()\r\n",
    "        total += labels.size(0)\r\n",
    "    print('Validation [Epoch: %d, %5d] loss: %.3f'%(epoch + 1, idx, val_loss / total))\r\n",
    "    print('Validation Acc: %.3f %%'%(val_corrects / total * 100))\r\n",
    "\r\n",
    "    # -------- Schedule Learning Rate --------\r\n",
    "    scheduler.step()\r\n",
    "    print(\"Learning Rate: \", scheduler.get_last_lr()[0])\r\n",
    "\r\n",
    "print('Finish Training')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/5236 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-5a6e72e1ad67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# get the input; data is a list of [inputs, labels]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# zero the parameter gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prepare to count predictions for each class\r\n",
    "correct_pred = {classname: 0 for classname in classes}\r\n",
    "total_pred = {classname: 0 for classname in classes}\r\n",
    "\r\n",
    "# again no gradients needed\r\n",
    "with torch.no_grad():\r\n",
    "    for data in test_loader:\r\n",
    "        images, labels = data\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "        outputs = model(images)\r\n",
    "        _, predictions = torch.max(outputs, 1)\r\n",
    "        # collect the correct predictions for each class\r\n",
    "        for label, prediction in zip(labels, predictions):\r\n",
    "            if label == prediction:\r\n",
    "                correct_pred[classes[label]] += 1\r\n",
    "            total_pred[classes[label]] += 1\r\n",
    "\r\n",
    "\r\n",
    "# print accuracy for each class\r\n",
    "for classname, correct_count in correct_pred.items():\r\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\r\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "74ea91778db1d856ff87590d1dff4d6dab0747003a22ee1eca989cea41e9e8d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}